{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OU69ut9uaYhR"
   },
   "source": [
    "# Mount Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9dLtB_JaVrG"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "PFlNCNaJyLNh",
    "outputId": "fda56ab3-f0a7-4a00-f291-c739116c82cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Input, Lambda,LSTM,Dropout,Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.optimizers as ko\n",
    "from keras.utils import plot_model\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import librosa as lb\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import freqz\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pickle\n",
    "import operator\n",
    "import IPython.display as ipd\n",
    "import itertools\n",
    "import numpy.random as rng\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xiE45ImqyrON"
   },
   "source": [
    "# Define Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frz9anw9cGve"
   },
   "outputs": [],
   "source": [
    "#Define Keras Model\n",
    "def LSTM_branch(input_shape):\n",
    "    input_seq = Input(shape=input_shape)\n",
    "    x = Bidirectional(LSTM(128,return_sequences=True),merge_mode='ave')(input_seq)\n",
    "    x = Bidirectional(LSTM(128,return_sequences=True),merge_mode='ave')(x)\n",
    "    x = Bidirectional(LSTM(128))(x)\n",
    "    x = Dense(128,activation='relu')(x)\n",
    "    x = Dense(128,activation='relu')(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    encoded = Activation(\"linear\")(x)\n",
    "    return Model(input_seq,encoded,name=\"LSTM\")\n",
    "\n",
    "# Loss and metrics\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    #return K.sqrt(K.sum(K.square(x - y), axis=-1, keepdims=True))\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() < 0.5\n",
    "    return np.mean(pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NsiXBx9uiwSZ"
   },
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcDwPfLjymZX"
   },
   "outputs": [],
   "source": [
    "def filter_by_freq(target,threshold):\n",
    "    filtered = dict()\n",
    "    for key in target:\n",
    "        if len(target[key]) >= threshold:\n",
    "            filtered[key] = target[key]\n",
    "    return filtered\n",
    "\n",
    "#Load support set from selection\n",
    "def create_support_set(sel_keys,selection,total,filtered,full):\n",
    "    support_set = dict()\n",
    "    for i in range(0, len(sel_keys)):\n",
    "        support_set[sel_keys[i]] = filtered[sel_keys[i]][selection[i]]\n",
    "\n",
    "    #If true adds rare phrases (those with less than 12 instances)\n",
    "    if full:\n",
    "        #load support set for rare phrases (with less than 12 tokens)\n",
    "        rare_phrases = { k : total[k] for k in set(total) - set(filtered) }\n",
    "        for key in rare_phrases.keys():\n",
    "            support_set[key]= librosa.load(rare_phrases[key][0]) #Choose the first one\n",
    "    return support_set \n",
    "\n",
    "def remv_support_set(sel_keys,selection,filtered):\n",
    "    #Remove support set instances from filtered set. \n",
    "    new_filtered = copy.deepcopy(filtered)\n",
    "    for i in range(len(sel_keys)):\n",
    "        a = new_filtered[sel_keys[i]]\n",
    "        del a[selection[i]]\n",
    "        new_filtered[sel_keys[i]] = a\n",
    "    return new_filtered\n",
    "\n",
    "def split_set(new_filtered,train_size):\n",
    "  #Returns train and test set\n",
    "    train = dict()\n",
    "    test = dict()\n",
    "    for k in new_filtered.keys():\n",
    "        #train[k],test[k] = train_test_split(new_filtered[k],train_size=train_size, random_state=rand_state)\n",
    "        train[k],test[k] = train_test_split(new_filtered[k],train_size=train_size)\n",
    "    return train, test\n",
    "\n",
    "#Generate train set for k-shot learning\n",
    "def get_batch(dataset,k,n):\n",
    "    \"\"\"Create batch of 2*n pairs per class using up to k examples, n same class, n different class\"\"\"\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    categories = dataset.keys()\n",
    "\n",
    "    #Create subset of dataset with only k elements per class\n",
    "    k_set = dict()\n",
    "    for cat in categories:\n",
    "        k_set[cat] = random.sample(dataset[cat],k) #Take k samples with no replacement per class\n",
    "\n",
    "    for i in range(n):\n",
    "        for cat in categories:\n",
    "            z1, z2 = random.choice(k_set[cat]), random.choice(k_set[cat])\n",
    "            pairs += [[z1,z2]] #Same class pair\n",
    "            \n",
    "            #Pick a a different category than current \"cat\"\n",
    "            while True:   \n",
    "                notcat = random.choice(list(categories))\n",
    "                if(notcat != cat):  \n",
    "                    break  \n",
    "            z1, z2 = random.choice(k_set[cat]), random.choice(k_set[notcat])\n",
    "            pairs += [[z1,z2]] #different class pair\n",
    "            labels += [1, 0] #1 to same pairs, 0 to contrastive\n",
    "    return np.array(pairs), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pW3B3mQLMtCM"
   },
   "source": [
    "# Load features from all phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqlAngYlL8Dj"
   },
   "outputs": [],
   "source": [
    "with open(\"features_total.pkl\", \"rb\") as input_file:\n",
    "    total_features = pickle.load(input_file)\n",
    "\n",
    "#Transpose vectors and compute decibels\n",
    "total_features_db = dict()\n",
    "for k in total_features.keys():\n",
    "    for i in range(len(total_features[k])):\n",
    "        total_features[k][i] = lb.amplitude_to_db(total_features[k][i],top_db=65.0)\n",
    "        total_features[k][i] = total_features[k][i].astype('int8')\n",
    "\n",
    "#Get most common phrases\n",
    "filt_features = filter_by_freq(total_features,12)\n",
    "total_features = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zpUfSNUhW_p"
   },
   "source": [
    "# Support Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "onX_JvWdhaeZ"
   },
   "source": [
    "Option 1: Support set is average of phrases (It works better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJMITA6Zfip_"
   },
   "outputs": [],
   "source": [
    "#Create support set from averages\n",
    "support_set = dict()\n",
    "for k in filt_features.keys():\n",
    "    support_set[k] = np.mean(filt_features[k],axis=0)\n",
    "support_set_array = np.array([s for s in list(support_set.values())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ja9hcxozgJcY"
   },
   "source": [
    "# Classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FLV0BAUYgHVo"
   },
   "outputs": [],
   "source": [
    "#Create classification set\n",
    "def create_classif_task(test_set):\n",
    "    classif_test = []\n",
    "    classif_labels = []\n",
    "\n",
    "    #use the full test set\n",
    "    for k in test_set.keys():\n",
    "        for a in test_set[k]:\n",
    "            classif_test.append(a)\n",
    "            classif_labels.append(k)\n",
    "    return (np.array(classif_test),classif_labels)\n",
    "\n",
    "def get_predictions(support_set,classif_test,model):\n",
    "    predictions = []\n",
    "    support_set_array = np.array([s for s in list(support_set.values())])\n",
    "    classif_test_repeated = np.repeat(classif_test,len(support_set_array),axis=0)\n",
    "    I, L = pd.factorize(list(support_set.keys()))\n",
    "    for k in range(len(classif_test)):\n",
    "        pred_support = model.predict([classif_test_repeated[32*k:32+32*k],support_set_array]).ravel()\n",
    "        pred_class = np.where(pred_support == np.min(pred_support))[0][0]\n",
    "        predictions.append(L[pred_class])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-MXnC_E1zuy"
   },
   "source": [
    "# Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRYtqp4G15TF"
   },
   "outputs": [],
   "source": [
    "def train_model(x,y,labels,epochs):\n",
    "    \"Creates, trains and returns trained model\"\n",
    "    input_shape = (64,128) #(Timesteps,n_features)\n",
    "    lstm = LSTM_branch(input_shape)\n",
    "    inputA = Input(shape=input_shape,name=\"InputA\")\n",
    "    inputB = Input(shape=input_shape,name=\"InputB\")\n",
    "    encodedA = lstm(inputA)\n",
    "    encodedB = lstm(inputB)\n",
    "    distance = Lambda(euclidean_distance,output_shape=eucl_dist_output_shape,name=\"distance\")([encodedA, encodedB])\n",
    "    model = Model(input=[inputA,inputB],output=distance)\n",
    "    model.compile(optimizer='adam', loss=contrastive_loss)\n",
    "    history = model.fit([x, y],labels,epochs=epochs,batch_size=256,shuffle=True)\n",
    "    return model, history.history['loss']\n",
    "\n",
    "def generate_sets(k):\n",
    "#Generate train_test set\n",
    "    train_set, test_set = split_set(filt_features,k)\n",
    "    train_pairs, train_labels =  get_batch(train_set,k,1000)\n",
    "    min_phrases_test = min([len(i) for i in test_set.values()])\n",
    "    test_pairs, test_labels = get_batch(test_set,min_phrases_test,100)\n",
    "    te1 = test_pairs[:,0,:,:]\n",
    "    te2 = test_pairs[:,1,:,:]\n",
    "    tr1 = train_pairs[:,0,:,:]\n",
    "    tr2 = train_pairs[:,1,:,:]\n",
    "    return tr1,tr2,train_labels,train_set,te1,te2,test_labels,test_set\n",
    "\n",
    "def compute_one_run(k,epochs):\n",
    "    tr1,tr2,train_labels,train_set,te1,te2,test_labels,test_set = generate_sets(k)\n",
    "    model, history = train_model(tr1,tr2,train_labels,epochs)\n",
    "\n",
    "    #Verification task evaluation (test)\n",
    "    v_pred_te = model.predict([te1,te2])\n",
    "    v_acc_te = compute_accuracy(test_labels,v_pred_te)\n",
    "\n",
    "    #Verification task evaluation (train)\n",
    "    v_pred_tr = model.predict([tr1,tr2])\n",
    "    v_acc_tr = compute_accuracy(train_labels,v_pred_tr)\n",
    "\n",
    "    #Classification task evaluation (test)\n",
    "    classif_test, classif_labels_test = create_classif_task(test_set)\n",
    "    predictions_test = get_predictions(support_set,classif_test,model)\n",
    "    c_acc_te = np.mean([predictions_test[i] == classif_labels_test[i] for i in range(len(predictions_test))])\n",
    "    \n",
    "    #Classification task evaluation (train)\n",
    "    classif_train, classif_labels_train = create_classif_task(train_set)\n",
    "    predictions_train = get_predictions(support_set,classif_train,model)\n",
    "    c_acc_tr = np.mean([predictions_train[i] == classif_labels_train[i] for i in range(len(predictions_train))])\n",
    "\n",
    "    #Accuracy per class (test)\n",
    "    acc_c_class_test = dict()\n",
    "    for k in test_set.keys():\n",
    "        k_indices = list(filter(lambda x: classif_labels_test[x] == k, range(len(classif_labels_test))))\n",
    "        acc_c_class_test[k] = np.mean([predictions_test[i] == classif_labels_test[i] for i in k_indices])\n",
    "    \n",
    "    #Accuracy per class (train)\n",
    "    acc_c_class_train = dict()\n",
    "    for k in train_set.keys():\n",
    "        k_indices = list(filter(lambda x: classif_labels_train[x] == k, range(len(classif_labels_train))))\n",
    "        acc_c_class_train[k] = np.mean([predictions_train[i] == classif_labels_train[i] for i in k_indices])\n",
    "\n",
    "    return (v_acc_tr,v_acc_te,c_acc_tr,c_acc_te,acc_c_class_train,acc_c_class_test,history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yn6sY6CjckCl"
   },
   "source": [
    "# Get and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "id": "wEkUA-d9ALBb",
    "outputId": "5dbe187b-e3c9-4e76-9ae7-26da8a089d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 1 from 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiago/anaconda3/envs/tf2-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"di...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "64000/64000 [==============================] - 117s 2ms/step - loss: 0.0967\n",
      "Epoch 2/5\n",
      "64000/64000 [==============================] - 103s 2ms/step - loss: 0.0579\n",
      "Epoch 3/5\n",
      "64000/64000 [==============================] - 104s 2ms/step - loss: 0.0524\n",
      "Epoch 4/5\n",
      "64000/64000 [==============================] - 103s 2ms/step - loss: 0.0502\n",
      "Epoch 5/5\n",
      "64000/64000 [==============================] - 104s 2ms/step - loss: 0.0486\n"
     ]
    }
   ],
   "source": [
    "H = []\n",
    "n = 1\n",
    "for i in range(n):\n",
    "    print(\"Experiment: \" + str(i+1) + \" from \" + str(n))\n",
    "    X = compute_one_run(k=7,epochs=5)\n",
    "    H.append(X)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFRUg4ljbzwi"
   },
   "outputs": [],
   "source": [
    "with open('k7.pickle', 'wb') as f:\n",
    "    pickle.dump(H, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2F0BI-fgcV2k"
   },
   "outputs": [],
   "source": [
    "#x is an array containing tuples of the form: \n",
    "#(v_acc_tr,v_acc_te,c_acc_tr,c_acc_te,acc_c_class_train,acc_c_class_test,history)\n",
    "with open('k7.pickle', 'rb') as f:\n",
    "    x = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "o9dLtB_JaVrG",
    "xiE45ImqyrON",
    "NsiXBx9uiwSZ",
    "pW3B3mQLMtCM",
    "1zpUfSNUhW_p"
   ],
   "name": "SiamesLSTM_ConfIntervals.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
